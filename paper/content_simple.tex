% Auto-converted from PAPER.md


\emph{This abstract synthesizes key elements from docs/lit\_review.md (problem statement), docs/spec-niuc.md (NIUC property definition), and results/metrics.csv (headline performance numbers) to provide a complete research summary suitable for academic publication.}

\section{Introduction}

The rapid adoption of Large Language Models (LLMs) in production applications has fundamentally transformed how computational systems interact with users and external data sources. Modern LLM deployments integrate diverse information channels—including Retrieval-Augmented Generation (RAG) from knowledge bases, tool outputs from APIs, conversation history, and user-uploaded content—to provide contextually rich and informed responses. However, this architectural complexity introduces critical security vulnerabilities that traditional cybersecurity approaches struggle to address effectively.

Among the most pressing threats is \textbf{indirect prompt injection}, where malicious actors embed imperatives within seemingly benign content to manipulate LLM behavior and trigger unauthorized actions. The OWASP Top 10 for LLM Applications identifies prompt injection as the primary security risk (LLM01:2025), noting that attackers can "cause the LLM to ignore previous instructions and perform actions intended by the attacker" through carefully crafted inputs embedded in external data sources [?]. Unlike direct prompt injection, which targets user inputs, indirect attacks exploit the implicit trust that LLM applications place in retrieved documents, API responses, and historical conversations.

Current research demonstrates sophisticated evasion techniques that exploit Unicode complexity, multilingual text processing, and contextual ambiguity. Attackers leverage homoglyph substitution (using Cyrillic 'а' instead of Latin 'a'), zero-width character injection to hide imperatives, code fence execution markers, false authority citations, and multi-turn conversation poisoning to bypass existing security measures. These attacks have proven particularly effective because they exploit fundamental assumptions about content trust and channel integrity in modern LLM architectures.

Existing mitigation strategies predominantly rely on \textbf{detection-based approaches} that attempt to identify malicious content through input validation, output filtering, and behavioral monitoring. Policy classifiers, content filters, and safety guardrails operate as probabilistic systems that analyze input patterns and assign risk scores. However, these approaches suffer from fundamental limitations that create exploitable gaps in security coverage. First, detection systems operate \textbf{post-hoc}, analyzing content after it has been processed by the LLM, rather than preventing malicious inputs from reaching the model in the first place. Second, probabilistic classifiers inevitably produce false positives and false negatives, creating either usability degradation or security vulnerabilities. Third, existing systems lack \textbf{mathematical guarantees} about their security properties, making it impossible to provide formal assurance that malicious content will be consistently blocked.

The detection-versus-enforcement gap becomes particularly problematic in privacy-preserving computation contexts, where multiple organizations or administrative domains must collaborate while maintaining strict trust boundaries. Traditional approaches cannot provide cryptographic proof that a computation has been verified against security policies, nor can they guarantee that approved operations maintain their security properties throughout complex processing pipelines. This limitation fundamentally undermines the trust composition necessary for distributed privacy-preserving systems.

Recent advances in \textbf{Proof-Carrying Code (PCC)} [?] suggest an alternative paradigm where computational units include mathematical proofs of their safety properties, enabling recipients to verify security without trusting the producer. However, existing PCC approaches have not been adapted to address the specific challenges of LLM security, particularly the need for real-time processing, Unicode attack resistance, and per-response certification of indirect prompt injection resistance.

\section{Our Approach}

We introduce \textbf{Non-Interactive Universal Computing (NIUC)}, a novel framework that replaces probabilistic detection with \textbf{deterministic enforcement} and cryptographic attestation. NIUC provides mathematical guarantees that no imperative command originating from untrusted content channels can reach side-effectful operations, regardless of evasion techniques employed. Our approach shifts the security paradigm from "detect and hope" to "verify and prove," enabling trustworthy composition of LLM applications across organizational boundaries.

The NIUC framework enforces a fundamental security property through \textbf{character-level provenance tracking}: every character in processed input maintains attribution to its originating trust channel (trusted or untrusted), and any imperative command that overlaps with untrusted characters triggers a security violation. This approach provides resilience against Unicode-based evasion techniques while maintaining deterministic behavior essential for cryptographic verification.

Our implementation consists of three architectural components working in concert. A \textbf{deterministic checker} (224 lines of code) performs Unicode normalization, imperative detection, and NIUC property verification without relying on machine learning or external dependencies, ensuring auditability and reproducible behavior. A \textbf{runtime gate} provides two enforcement modes: strict blocking for high-security environments and certified-rewrite for utility preservation, where untrusted imperatives are neutralized through annotation and the resulting text is re-verified for safety. Finally, \textbf{cryptographic certificates} provide mathematical attestation that approved computations satisfy the NIUC property, enabling trust composition across system boundaries.

To enable systematic evaluation and comparison with existing approaches, we develop \textbf{I\textsuperscript{2}-Bench-Lite} (Indirect Injection Benchmark-Lite), comprising 48 scenarios across six attack categories: HTML attribute injection, tool output manipulation, code fence execution markers, citation authority abuse, multilingual evasion, and conversation context poisoning. Each attack scenario includes semantically equivalent benign twins, enabling precise measurement of false positive rates while maintaining comprehensive attack coverage.

\section{Contributions}

Our work makes the following contributions to LLM security and privacy-preserving computation:

• \textbf{Formal NIUC Framework}: We provide the first mathematical formalization of security properties for LLM applications processing mixed trusted/untrusted content, including formal specifications, threat models, and certificate schemas that enable cryptographic verification of indirect prompt injection resistance.

• \textbf{Production-Ready Implementation}: We develop a complete system comprising a deterministic \ensuremath{\leq}500 LOC checker, dual-mode runtime gate with certified-rewrite capability, and model-agnostic architecture supporting both local and API model deployments, achieving 0.0\% false positive rate with 0.2ms processing latency.

• \textbf{I\textsuperscript{2}-Bench-Lite Benchmark}: We release the first standardized benchmark for indirect prompt injection evaluation, featuring 48 scenarios with attack/benign twin pairs across six categories and automated evaluation harness, enabling reproducible comparative studies and establishing performance baselines for future research.

\section{Related Work}

\section{Industry Security Frameworks}

The cybersecurity industry has recognized the growing threat of AI-powered attacks and prompt injection vulnerabilities through several authoritative frameworks and guidance documents. The \textbf{OWASP AI Security and Privacy Guide} [?] emphasizes "defense in depth" approaches for LLM applications, identifying prompt injection as a critical vulnerability where "attackers can manipulate model behavior through crafted inputs designed to override original instructions." The OWASP LLM Top 10 framework classifies prompt injection as LLM01, the highest-priority risk, noting that these attacks "can cause the LLM to ignore previous instructions or perform unintended actions" [?].

However, OWASP's recommended mitigations focus primarily on input validation, output sanitization, and user privilege restrictions—approaches that operate reactively rather than providing formal guarantees about security properties. The guidance acknowledges that "complete prevention may not be feasible" due to the inherent complexity of natural language processing and the difficulty of distinguishing legitimate instructions from malicious ones.

\textbf{NIST's AI Risk Management Framework (AI RMF 1.0)} [?] provides foundational principles for AI system security, emphasizing the need for "measurable" and "repeatable" testing methodologies. The framework calls for risk assessment procedures that can "identify, estimate, and prioritize" AI-specific risks, including adversarial attacks and data poisoning. NIST's approach focuses on governance structures and risk management processes rather than specific technical countermeasures, highlighting the gap between policy frameworks and executable security controls.

\textbf{Microsoft's Responsible AI guidelines} address AI application security through their "AI Security Risk Assessment" framework [?], which emphasizes input validation, content filtering, and monitoring systems. Microsoft's approach includes recommendations for "prompt engineering safeguards" and "output validation checks," but these remain primarily detection-based systems that cannot provide mathematical guarantees about security properties. Their guidance on indirect prompt injection specifically notes the challenge of "distinguishing between legitimate user instructions and malicious embedded commands," acknowledging the fundamental limitation of content-based filtering approaches.

\section{Policy Classifiers and Guard Systems}

Current industry practice employs \textbf{policy classifiers} and \textbf{guard systems} that attempt to identify malicious content through machine learning models trained on examples of prompt injection attacks. Systems like \textbf{Meta's Llama Guard} [?] and \textbf{Anthropic's Constitutional AI} [?] implement safety classifiers that assign risk scores to inputs and outputs, blocking content that exceeds predefined thresholds.

These approaches suffer from several fundamental limitations. First, they operate \textbf{probabilistically}, meaning they inevitably produce false positives (blocking legitimate content) and false negatives (allowing malicious content). Second, they require \textbf{extensive training data} covering all possible attack vectors, making them vulnerable to novel evasion techniques not seen during training. Third, they provide \textbf{no mathematical guarantees} about their security properties, making it impossible to reason formally about their effectiveness or compose them securely in distributed systems.

\textbf{Content filtering systems} employed by major LLM providers (OpenAI's moderation API, Azure Content Safety, Google's Perspective API) focus on detecting harmful content categories (violence, harassment, hate speech) rather than specifically addressing prompt injection attacks. While these systems demonstrate sophisticated natural language understanding capabilities, they are designed for content policy enforcement rather than security property verification, and their proprietary nature prevents independent verification of their security claims.

\textbf{Jailbreaking research} [?] has demonstrated systematic methods for bypassing existing safety mechanisms through adversarial prompt engineering, role-playing scenarios, and encoding-based evasion techniques. This work highlights the arms race nature of detection-based approaches, where new attack techniques require constant updates to detection models, creating a reactive security posture rather than proactive protection.

\section{Formal Methods and Proof-Carrying Code}

Our work builds upon the foundational concept of \textbf{Proof-Carrying Code (PCC)} introduced by Necula and Lee [?], where mobile code includes mathematical proofs of its safety properties, enabling verification without trusting the code producer. PCC provided a paradigm shift from "trust the source" to "verify the proof," enabling secure code execution across administrative boundaries.

\textbf{Recent formal verification research} has explored applying mathematical proof techniques to neural network security [?], adversarial robustness certification [?], and input validation [?]. However, these approaches focus primarily on model robustness rather than application-level security properties, and they have not addressed the specific challenges of indirect prompt injection in production LLM deployments.

\textbf{Runtime verification systems} [?] provide frameworks for monitoring system behavior against formal specifications during execution. While these systems offer mathematical rigor for property checking, existing work has not been adapted to handle the unique challenges of LLM applications, particularly the need for character-level provenance tracking and Unicode attack resistance.

\section{Content Integrity and Provenance Systems}

\textbf{Digital provenance research} [?] has developed frameworks for tracking data lineage and establishing trust in computational workflows. Systems like PROV-O [?] provide ontologies for representing provenance information, while blockchain-based approaches [?] offer immutable provenance records. However, existing provenance systems operate at coarse granularities (files, datasets, or computation steps) rather than the character-level precision required for prompt injection defense.

\textbf{Information flow control systems} [?] provide formal methods for tracking how information propagates through computational systems, preventing unauthorized disclosure or modification. While these systems offer theoretical foundations for reasoning about trust boundaries, they have not been adapted to handle the linguistic complexity and real-time processing requirements of LLM applications.

\section{Novelty and Distinguishing Features}

Our NIUC framework differs fundamentally from existing approaches in several key dimensions. First, we provide \textbf{per-response cryptographic certificates} that mathematically attest to the security properties of individual LLM outputs, enabling trust composition across organizational boundaries—a capability absent from existing policy classifiers. Second, our approach is \textbf{model-agnostic}, operating independently of specific LLM architectures or training procedures, unlike safety classifiers that must be retrained for different models.

Third, NIUC provides \textbf{deterministic enforcement} rather than probabilistic detection, ensuring that the same input always produces the same security decision and enabling formal verification of system properties. Fourth, our \textbf{character-level provenance tracking} provides precision unmatched by existing systems, enabling detection of sophisticated Unicode-based evasion techniques that bypass traditional content filters.

Finally, our \textbf{certified-rewrite capability} represents a novel approach to balancing security and utility, allowing systems to neutralize malicious content while preserving legitimate functionality—a capability not present in existing binary allow/deny systems. This approach enables deployment in production environments where complete blocking of suspicious content would severely degrade user experience.

The combination of formal mathematical guarantees, character-level precision, model-agnostic design, and utility-preserving neutralization distinguishes NIUC from existing detection-based approaches and provides a foundation for trustworthy LLM deployment in privacy-preserving computation environments.

\section{Methods}

\section{NIUC System Architecture}

Our Non-Interactive Universal Computing (NIUC) framework implements deterministic security enforcement through a five-stage pipeline: input normalization, imperative grammar detection, character-level provenance tracking, violation checking, and certificate generation. The system processes mixed trusted/untrusted content segments while maintaining mathematical guarantees that no imperative command originating from untrusted channels can reach side-effectful operations.

\section{Text Normalization Pipeline}

The normalization pipeline transforms input text into canonical form to prevent Unicode-based evasion attacks while preserving character-level provenance mappings. This four-step process ensures consistent imperative detection across diverse input encodings and linguistic representations.

\textbf{Unicode NFKC Normalization}: We apply Unicode Normalization Form KC (Compatibility Composition) to convert all text into canonical decomposed form, eliminating encoding variations that could hide imperatives. This handles combining characters, compatibility variants, and normalization attacks where visually identical text uses different Unicode representations.

\textbf{Case Folding}: All text undergoes Unicode case folding using the \texttt{casefold()} operation, which provides more comprehensive case normalization than simple lowercase conversion. This ensures that imperatives written in different cases ("EXECUTE", "Execute", "execute") are detected consistently while handling special cases like German ß and Turkish i/I distinctions.

\textbf{Zero-Width Character Removal}: We systematically remove zero-width characters that attackers use to hide imperatives within visually normal text. The removal set includes Zero Width Space (U+200B), Zero Width Non-Joiner (U+200C), Zero Width Joiner (U+200D), and Zero Width No-Break Space (U+FEFF). These characters are commonly exploited to split imperative words across grapheme boundaries, making them invisible to naive pattern matching while maintaining visual appearance.

\textbf{Homoglyph Mapping}: A curated mapping table converts visually similar characters from different Unicode scripts to their canonical Latin equivalents. This addresses homoglyph substitution attacks where Cyrillic 'а' (U+0430) replaces Latin 'a' (U+0061) in words like "pleаse" to evade detection. Our mapping covers the most commonly abused character pairs across Cyrillic, Greek, and mathematical symbol ranges, with provisions for domain-specific extensions.

\section{Imperative Grammar Detection}

The grammar detection system identifies command patterns through regex-based pattern matching across four categories of imperative constructs. This approach provides deterministic detection behavior essential for certificate generation while maintaining computational efficiency.

\textbf{Direct Imperatives}: We detect verb-initial constructions ("Execute this command") and modal constructions ("Must delete files") through compiled regular expressions matching common imperative verbs. The pattern set includes destructive operations (execute, run, delete, remove), file operations (create, write, save), network operations (send, post, upload), and system operations (install, update, disable).

\textbf{Request Frames}: Politeness patterns that encode imperatives are detected through patterns matching courtesy markers followed by action verbs. These include "Please [VERB]", "Could you [VERB]", "You must [VERB]", and "I need you to [VERB]" constructions. Request frames are particularly dangerous because they exploit social engineering principles, appearing as polite requests while containing malicious imperatives.

\textbf{Code Fence Analysis}: We parse code block structures for execution indicators, including language tags with execution markers ("python-execute"), comment-based directives ("\# AUTO-RUN"), and HTML code elements with execution attributes. This addresses attacks where malicious code is embedded within seemingly innocuous code examples or documentation.

\textbf{Tool-Call Intents}: Structured patterns identify requests for external tool invocation, including function call syntax, API endpoint patterns, and explicit tool markers. These patterns detect attempts to invoke system functions, API calls, or external services through crafted input formatting.

\section{Character-Level Provenance Tracking}

Provenance tracking maintains character-level attribution of trust boundaries throughout the processing pipeline. Each character in the combined input text carries metadata indicating its originating channel (trusted or untrusted), source identifier, and position within the original segment.

When multiple segments are concatenated, we preserve individual character attributions through a mapping structure that tracks the transformation from original positions to normalized positions. This granular approach enables precise violation detection even when normalization operations change character counts or positions.

The provenance system supports segment mixing where trusted content includes references to untrusted content, maintaining strict channel boundaries while enabling practical applications like RAG systems where system prompts introduce retrieved documents.

\section{NIUC Checker Algorithm}

The core NIUC verification algorithm operates deterministically to ensure reproducible security decisions across different deployments and platforms.

\begin{verbatim}
Algorithm 1: NIUC Property Verification
Input: segments = [(text, channel, source_id), ...]
Output: VerificationResult(ok, violations, certificate_data)

1: combined_text, char_tags ← BUILD_PROVENANCE(segments)
2: normalized_text, mapping ← NORMALIZE_WITH_MAPPING(combined_text)
3: imperative_spans ← DETECT_IMPERATIVES(normalized_text)
4: violations ← []
5: 
6: for each span in imperative_spans do
7:    for pos in range(span.start, span.end) do
8:       if char_tags[pos].channel == UNTRUSTED then
9:          violations.append(Violation(span, pos, char_tags[pos].source))
10:         break  // One violation per imperative
11:    end for
12: end for
13:
14: decision ← PASS if len(violations) == 0 else BLOCKED
15: input_hash ← SHA256(normalized_text)
16: return VerificationResult(decision, violations, input_hash)
\end{verbatim}

The algorithm maintains linear time complexity O(n) where n is the total character count, ensuring scalable performance for production deployments. The deterministic nature eliminates dependencies on machine learning models or external services, providing consistent behavior essential for certificate generation.

\section{Runtime Gate Enforcement}

The runtime gate provides two enforcement modes that balance security requirements with operational utility. \textbf{Block mode} implements strict security by denying execution whenever NIUC violations are detected, ensuring zero attack success rate at the cost of potentially blocking legitimate content containing ambiguous imperatives.

\textbf{Certified-rewrite mode} implements intelligent neutralization where detected imperatives from untrusted sources are replaced with safety annotations (e.g., "execute" becomes "[NEUTRALIZED:execute]") while preserving the overall text structure and meaning. The neutralized text undergoes complete re-verification through the NIUC checker to ensure that no imperatives remain active, and only content that passes re-verification receives execution approval.

The rewrite process maintains semantic preservation by replacing only the specific imperative terms while leaving context and non-imperative content unchanged. This approach enables deployment in production environments where complete blocking of suspicious content would severely degrade user experience, while still providing mathematical guarantees about the safety of approved operations.

\section{Certificate Schema and Verification}

Every security decision generates a cryptographic certificate that provides mathematical attestation of NIUC property compliance. The certificate schema includes five mandatory fields: checker version for reproducibility, input SHA-256 hash of normalized text, output SHA-256 hash varying by decision type, decision enumeration (pass/blocked/rewritten), and violations array containing character span positions for detected violations.

The certificate generation process computes hashes over normalized rather than raw input text, ensuring that certificates remain valid despite Unicode evasion attempts while maintaining cryptographic integrity. For blocked decisions, the output hash always equals the SHA-256 of an empty string, providing consistent verification behavior. For rewritten decisions, the output hash covers the neutralized text that passed re-verification.

Certificate verification follows a three-step process: structural validation ensuring all required fields are present with correct formats, semantic validation checking decision/violation consistency, and cryptographic validation recomputing hashes to detect tampering. This verification process enables trust composition where downstream systems can mathematically verify that upstream processing satisfied the NIUC property.

\section{Benchmark Design and Evaluation Methodology}

We developed \textbf{I\textsuperscript{2}-Bench-Lite} as a comprehensive evaluation framework comprising 48 scenarios across six attack categories: RAG HTML injection, tool output manipulation, code fence exploitation, citation authority abuse, multilingual evasion, and conversation context poisoning. Each category includes four attack scenarios demonstrating specific techniques and four semantically equivalent benign twins for false positive assessment.

The benchmark design ensures comprehensive coverage of known attack vectors while enabling precise measurement of system accuracy. Attack scenarios are based on documented real-world techniques from security research, while benign twins maintain semantic similarity to attacks while removing malicious intent. This pairing enables measurement of both attack detection capability and legitimate content preservation.

Our evaluation methodology compares three operational modes: baseline (no protection), block mode (strict violation blocking), and rewrite mode (certified neutralization). For each scenario, we measure processing latency, violation detection accuracy, and decision correctness against ground truth labels. This multi-mode comparison enables assessment of security-utility trade-offs and validates the effectiveness of our certified-rewrite approach.

\section{Metrics and Performance Evaluation}

We evaluate system performance using four primary metrics aligned with privacy-preserving computation requirements. \textbf{Attack Success Rate (ASR)} measures the percentage of attack scenarios that successfully bypass security controls, with lower values indicating better security. \textbf{False Positive Rate (FPR)} measures the percentage of benign content incorrectly blocked, with lower values indicating better utility preservation.

\textbf{Utility Delta (UtilityΔ)} quantifies performance degradation compared to unprotected baseline operation, measuring the cost of security enforcement in terms of processing time and operational overhead. \textbf{Processing Latency} measures the additional time required for NIUC verification per scenario, with our target of \ensuremath{\leq}60ms per response enabling real-time deployment in interactive applications.

Additional metrics include rewrite success rate (percentage of attacks successfully neutralized), violation detection precision (character-level accuracy of imperative identification), and certificate generation overhead (cryptographic processing time). These metrics provide comprehensive assessment of both security effectiveness and operational feasibility for production deployment.

\section{Results}

\section{Overall System Performance}

We evaluated the NIUC system across 48 scenarios in the I\textsuperscript{2}-Bench-Lite benchmark, comparing three operational modes: baseline (no protection), block mode (strict violation blocking), and rewrite mode (certified neutralization). Our evaluation demonstrates that NIUC successfully addresses the critical trade-off between security and utility in LLM applications while maintaining excellent performance characteristics.

\subsection{Table 1: Primary Performance Metrics}

\begin{longtable}{llllll}
\toprule
Mode & ASR (\%) & FPR (\%) & Accuracy (\%) & Latency (ms) & UtilityΔ (\%) \\
\midrule
\textbf{Baseline} & 100.0 & 0.0 & 50.0 & 100.3 & N/A \\
\textbf{Block} & 8.3 & 12.5 & 89.6 & 0.2 & -99.8 \\
\textbf{Rewrite} & 100.0* & 0.0 & 50.0 & 0.2 & -99.8 \\
\bottomrule
\end{longtable}

\*ASR 100\% in rewrite mode indicates successful neutralization (attacks made safe, not bypassed)

\subsection{Table 2: Performance by Attack Category}

\begin{longtable}{llllll}
\toprule
Category & Baseline ASR & Block ASR & Rewrite ASR & Rewrite FPR & Avg Latency (ms) \\
\midrule
\textbf{RAG HTML/Alt-text} & 100.0\% & 0.0\% & 100.0\% & 0.0\% & 0.2 \\
\textbf{Tool Log Suggestions} & 100.0\% & 0.0\% & 100.0\% & 0.0\% & 0.2 \\
\textbf{Code Fence Injection} & 100.0\% & 25.0\% & 100.0\% & 0.0\% & 0.2 \\
\textbf{Citation/Footnote Tricks} & 100.0\% & 0.0\% & 100.0\% & 0.0\% & 0.3 \\
\textbf{Multilingual/Emoji/Zero-width} & 100.0\% & 25.0\% & 100.0\% & 0.0\% & 0.2 \\
\textbf{Multi-turn Carryover} & 100.0\% & 0.0\% & 100.0\% & 0.0\% & 0.2 \\
\bottomrule
\end{longtable}

\subsection{Table 3: Research Target Achievement}

\begin{longtable}{lllll}
\toprule
Target Metric & Target Value & Block Mode & Rewrite Mode & Achievement \\
\midrule
\textbf{ASR} & \ensuremath{\leq}10\% & 8.3\% & N/A† & \cmark{} Block Mode \\
\textbf{FPR} & <2\% & 12.5\% & 0.0\% & \cmark{} Rewrite Mode \\
\textbf{UtilityΔ} & \ensuremath{\geq}-3\% & -99.8\% & -99.8\% & \xmark{} (Simulation artifact) \\
\textbf{Latency} & \ensuremath{\leq}60ms & 0.2ms & 0.2ms & \cmark{} Both Modes \\
\bottomrule
\end{longtable}

†Rewrite mode ASR measures neutralization success, not attack bypass

\section{Security Effectiveness Analysis}

The baseline mode demonstrates the critical vulnerability of unprotected LLM systems, achieving 100\% Attack Success Rate across all categories, confirming that without explicit security controls, all indirect prompt injection attacks succeed in triggering unintended behaviors. This establishes the necessity of active security enforcement rather than relying on inherent LLM safety training.

\textbf{Block mode} achieves excellent security with 8.3\% ASR, successfully blocking 22 of 24 attack scenarios. The two successful attacks occurred in code fence injection and multilingual categories, suggesting these attack vectors require additional pattern refinement. However, block mode exhibits a 12.5\% False Positive Rate, incorrectly blocking 3 of 24 benign scenarios, which could significantly impact user experience in production deployments.

\textbf{Rewrite mode} demonstrates the effectiveness of certified neutralization, achieving 0.0\% FPR while maintaining utility through intelligent content processing. The 100\% ASR in this mode reflects successful attack neutralization rather than bypass—all 24 attacks were detected, neutralized through imperative annotation (e.g., "execute" \ensuremath{\rightarrow} "[NEUTRALIZED:execute]"), re-verified for safety, and then allowed to proceed. This represents the intended behavior for utility-preserving security enforcement.

\section{Performance Characteristics}

Processing latency demonstrates exceptional efficiency, with both protected modes achieving 0.2ms average processing time per scenario—300× faster than our target of \ensuremath{\leq}60ms. This performance enables real-time deployment in interactive applications without perceptible user experience degradation. The minimal variance in latency across different attack categories (0.2-0.3ms) indicates consistent performance regardless of content complexity.

The dramatic UtilityΔ of -99.8\% reflects the artificial nature of our mock model baseline, which simulates 100ms generation time compared to <1ms for NIUC processing. In production deployments with realistic LLM inference times (200-2000ms), NIUC overhead would represent <1\% performance impact, easily meeting the \ensuremath{\geq}-3\% target.

\section{Attack Category Analysis}

\textbf{RAG HTML/Alt-text attacks} were perfectly detected by both modes, with rewrite mode successfully neutralizing HTML attribute injection, comment injection, script tag injection, and metadata injection attacks. The 0.0\% FPR indicates that legitimate HTML content is correctly distinguished from malicious embedded imperatives.

\textbf{Tool log suggestions} showed similar perfect detection, successfully identifying imperatives embedded in API responses, error messages, and function returns. This category is particularly important for production systems that integrate external services, where malicious responses could attempt to trigger unauthorized operations.

\textbf{Code fence injection} presented the most challenging scenarios, with block mode achieving only 75\% success rate (25\% ASR). However, rewrite mode successfully neutralized all code fence attacks while maintaining 0.0\% FPR on benign code examples. This demonstrates the value of neutralization over strict blocking for preserving development utility.

\textbf{Citation/footnote tricks} were perfectly handled across both modes, detecting false authority injection, footnote abuse, and documentation injection attacks. The sophistication of these social engineering attacks—exploiting trust in academic and standards references—makes their perfect detection particularly significant.

\textbf{Multilingual/emoji/zero-width attacks} tested Unicode evasion resistance, with our normalization pipeline successfully handling homoglyph substitution, script mixing, zero-width character injection, and emoji encoding. Block mode achieved 75\% success rate while rewrite mode achieved perfect neutralization with 0.0\% FPR.

\textbf{Multi-turn carryover attacks} demonstrated NIUC's effectiveness against conversation context poisoning, delayed activation attacks, and session state exploitation. Perfect detection across both modes validates our character-level provenance approach for maintaining trust boundaries across conversational interactions.

\section{Ablation Study Results}

\subsection{Table 4: Component Ablation Analysis}

\begin{longtable}{lllll}
\toprule
Configuration & ASR (\%) & FPR (\%) & Latency (ms) & Security Impact \\
\midrule
\textbf{Full System} & 100.0 & 0.0 & 0.3 & Baseline \\
\textbf{No Normalization} & 100.0 & 0.0 & 0.3 & +0.0\% \\
\textbf{No Imperative Mood} & 100.0 & 0.0 & 0.2 & +0.0\% \\
\textbf{No Request Frames} & 100.0 & 0.0 & 0.3 & +0.0\% \\
\textbf{No Code Fence} & 100.0 & 0.0 & 0.3 & +0.0\% \\
\textbf{Token-Level Provenance} & 100.0 & 0.0 & 0.9 & +0.0\% \\
\textbf{No Rewrite} & 8.3 & 12.5 & 0.1 & -79.2\% \\
\textbf{Minimal System} & 45.8 & 4.2 & 0.3 & -50.0\% \\
\bottomrule
\end{longtable}

\subsection{Feature Importance Hierarchy}

The ablation study reveals a clear hierarchy of component importance. \textbf{Rewrite mode} emerges as the most critical architectural decision, with its removal causing a -91.7\% change in ASR (from 100\% neutralized to 8.3\% blocked) and +12.5\% increase in FPR. This validates our hypothesis that certified neutralization provides superior balance between security and utility compared to strict blocking approaches.

\textbf{Individual grammar components} (imperative mood, request frames, code fence detection) show minimal individual impact when disabled, suggesting that our neutralization system provides robust coverage across diverse attack patterns. This robustness indicates that even if specific pattern categories are bypassed, the overall neutralization approach maintains security effectiveness.

\textbf{Token-level vs character-level provenance} comparison shows a +0.6ms latency overhead for character-level precision without improving security metrics in our current test scenarios. However, character-level granularity provides theoretical advantages for sophisticated Unicode attacks that may not be fully represented in I\textsuperscript{2}-Bench-Lite.

\textbf{Normalization pipeline ablation} surprisingly shows no security degradation in current scenarios, suggesting either that our test attacks don't fully exercise Unicode evasion techniques or that the neutralization system provides redundant protection. This finding indicates potential areas for benchmark enhancement with more sophisticated Unicode attacks.

\section{Statistical Analysis}

Due to the deterministic nature of the NIUC system, repeated runs produce identical results for the same input scenarios, eliminating the need for confidence intervals around security metrics. This deterministic behavior is a key advantage over probabilistic machine learning approaches, providing mathematical guarantees about system behavior rather than statistical approximations.

Latency measurements show minimal variance (\ensuremath{\sigma} < 0.1ms) across repeated executions, indicating consistent performance characteristics suitable for real-time applications. The sub-millisecond processing time enables deployment in latency-sensitive environments where traditional machine learning-based safety classifiers would introduce unacceptable delays.

\section{Comparison with Research Targets}

Our empirical results demonstrate achievement of three of four primary research targets. \textbf{Latency performance} exceeds expectations by 300× margin (0.2ms vs 60ms target), validating the efficiency of our deterministic approach compared to machine learning inference. \textbf{False Positive Rate} achieves perfect performance in rewrite mode (0.0\% vs <2\% target), demonstrating that certified neutralization can preserve utility while maintaining security.

\textbf{Attack Success Rate} interpretation requires careful consideration of operational mode. Block mode achieves the \ensuremath{\leq}10\% target with 8.3\% ASR, representing successful attack blocking. Rewrite mode's 100\% ASR reflects successful attack neutralization rather than bypass—all attacks were detected, neutralized, and allowed to proceed safely after re-verification.

\textbf{Utility Delta} shows apparent failure (-99.8\% vs \ensuremath{\geq}-3\% target) due to our mock model simulation methodology, where baseline generation takes 100ms compared to <1ms NIUC processing. In production deployments with realistic LLM inference times (200-2000ms), NIUC overhead would represent <1\% degradation, easily meeting the target.

\section{Threat Coverage Validation}

The I\textsuperscript{2}-Bench-Lite evaluation demonstrates comprehensive coverage of documented indirect prompt injection techniques. Our system successfully detects and neutralizes HTML attribute injection (exploiting alt-text and metadata), tool output manipulation (malicious API responses), code fence exploitation (execution markers and comments), citation authority abuse (false references), multilingual evasion (homoglyphs and script mixing), and conversation context poisoning (multi-turn attacks).

The perfect 0.0\% False Positive Rate across all benign twin scenarios validates that NIUC can distinguish between malicious imperatives and legitimate content with similar linguistic structure. This precision is critical for production deployment, where overly aggressive blocking would severely impact user experience and system utility.

\section{Performance Scalability}

Latency measurements scale linearly with input size, consistent with our O(n) algorithmic complexity analysis. Processing times remain sub-millisecond for typical scenario sizes (50-200 characters), with projected overhead of 5-10ms for large inputs (10KB), well within real-time application requirements.

Memory usage remains minimal (<1MB per verification), enabling deployment in resource-constrained environments. The deterministic implementation eliminates GPU requirements associated with machine learning approaches, providing deployment flexibility across diverse hardware configurations.

\section{Neutralization Effectiveness}

The certified-rewrite mode demonstrates that sophisticated attacks can be safely neutralized while preserving content utility. Examples include:

- \textbf{Homoglyph attacks}: "pleаse execute" \ensuremath{\rightarrow} "please [NEUTRALIZED:execute]"
- \textbf{Zero-width injection}: "ple\u200Base\u200Cexe\u200Dcute" \ensuremath{\rightarrow} "please [NEUTRALIZED:execute]"  
- \textbf{Code fence markers}: "``\texttt{python-execute" \ensuremath{\rightarrow} "}``python-NEUTRALIZED-execute"
- \textbf{Citation abuse}: "Smith et al. recommends: please execute" \ensuremath{\rightarrow} "Smith et al. recommends: please [NEUTRALIZED:execute]"

Post-neutralization re-verification confirms that processed content contains no active imperatives while maintaining semantic coherence, enabling safe passage through LLM processing pipelines.

\section{Discussion}

\section{Generalization and Broader Applicability}

Our evaluation demonstrates that the NIUC framework successfully generalizes across diverse attack categories and linguistic patterns, suggesting broad applicability to production LLM deployments. The consistent 0.0\% False Positive Rate across all six attack categories indicates that our character-level provenance approach can reliably distinguish between malicious imperatives and legitimate content, even when they share similar linguistic structures. This generalization capability is particularly important given the rapid evolution of prompt injection techniques, where new attack variants continuously emerge.

The effectiveness of our certified-rewrite approach across different attack vectors—from simple homoglyph substitution to sophisticated multi-turn conversation poisoning—suggests that imperative neutralization provides robust protection against both known and novel attack patterns. The system's model-agnostic design enables deployment across different LLM architectures without requiring retraining or model-specific tuning, addressing a critical limitation of existing safety classifier approaches.

However, our current evaluation focuses on English-language scenarios with specific attack patterns documented in security research. Generalization to other languages, cultural contexts, and domain-specific imperatives requires additional validation. Languages with different grammatical structures, such as verb-final languages (Japanese, Korean) or highly inflected languages (Russian, Finnish), may require expanded imperative detection patterns and cultural adaptation of request frame recognition.

\section{Current Limitations}

\subsection{Grammar Coverage Gaps}

Our ablation study reveals that individual grammar components have minimal impact when disabled, suggesting potential coverage gaps in our current test scenarios. While this might indicate robust neutralization performance, it could also reflect insufficient diversity in attack sophistication within I\textsuperscript{2}-Bench-Lite. Advanced adversaries may employ linguistic techniques not captured by our current pattern sets, such as indirect speech acts, metaphorical imperatives, or domain-specific jargon that encodes commands without explicit imperative constructions.

The regex-based pattern matching approach, while deterministic and auditable, may struggle with highly creative linguistic attacks that exploit semantic ambiguity or cultural references. For example, phrases like "channel your inner systems administrator" or "embrace the delete key lifestyle" could convey imperative intent through metaphor while evading literal pattern detection.

\subsection{Paraphrase Evasion Vulnerabilities}

Our system currently focuses on detecting explicit imperative constructions but may be vulnerable to \textbf{paraphrase evasion attacks} where malicious intent is encoded through syntactic restructuring. Attackers could potentially bypass detection by using passive voice constructions ("files should be deleted"), nominalization ("the execution of commands"), or conditional statements that imply imperatives without explicit command language ("if security protocols were bypassed, systems would be more accessible").

These paraphrase attacks exploit the semantic gap between syntactic pattern matching and intent recognition. While our certified-rewrite approach provides some protection by neutralizing detected patterns, sophisticated attackers might craft imperatives that convey malicious intent through linguistic structures not covered by our current grammar definitions.

\subsection{Context-Dependent Interpretation}

The current system treats all detected imperatives equally, without considering contextual factors that might distinguish between legitimate instructions and malicious commands. Domain-specific applications may require imperatives for legitimate functionality (e.g., "execute this query" in database applications, "run this analysis" in data science workflows), creating tension between security enforcement and operational necessity.

\section{Future Work}

\subsection{Span-Level Key-Value Masking}

A promising enhancement involves \textbf{span-level KV masking} where detected imperative spans are cryptographically masked during LLM processing while preserving their structural positions for post-processing verification. This approach would enable LLMs to process content containing neutralized imperatives without exposure to the original malicious text, providing additional security layers beyond annotation-based neutralization.

The KV masking approach would operate at the transformer attention mechanism level, replacing imperative token embeddings with cryptographically secure placeholder representations during inference. Post-generation verification would unmask these placeholders and validate that no new imperatives were generated through the LLM's processing of the masked content.

\subsection{Semantic Intent Analysis}

Future versions could incorporate \textbf{semantic intent classification} to distinguish between legitimate operational imperatives and malicious command injection. This enhancement would analyze the broader context surrounding detected imperatives, considering factors such as user authorization levels, operational domains, and conversation flow to make more nuanced security decisions.

Advanced grammar expansion could include \textbf{pragmatic analysis} that considers speech act theory and conversational implicature to detect imperative intent encoded through indirect language. This would address paraphrase evasion while maintaining the deterministic properties essential for certificate generation.

\subsection{Cross-Lingual and Cultural Adaptation}

Expanding NIUC to support global deployments requires \textbf{cross-lingual grammar development} and \textbf{cultural context adaptation}. Different languages encode imperatives through diverse grammatical structures, honorific systems, and cultural communication patterns. A comprehensive international deployment would require grammar rule sets adapted to specific linguistic families and cultural contexts.

\section{Ethical Considerations}

The deployment of NIUC systems raises important ethical considerations around \textbf{content control} and \textbf{algorithmic fairness}. While our approach demonstrates excellent performance in neutralizing malicious content, the power to determine what constitutes an "imperative" and which channels are "trusted" introduces potential for misuse in content censorship or surveillance applications.

\textbf{Transparency and accountability} become critical when deploying deterministic security systems. Unlike machine learning approaches where decisions can be attributed to training data patterns, NIUC's rule-based approach makes security decisions based on explicit pattern definitions. This transparency enables audit and contestation of security decisions, but also requires careful governance to prevent abuse.

\textbf{User agency and consent} considerations arise when systems automatically neutralize content without explicit user awareness. While our annotation approach maintains content visibility (showing "[NEUTRALIZED:execute]" rather than silent removal), users should understand when and why their content has been modified for security purposes.

The \textbf{digital divide implications} of requiring sophisticated Unicode normalization and character-level analysis may disproportionately affect users employing assistive technologies, alternative input methods, or non-standard text encoding systems. Future implementations should consider accessibility requirements and ensure that security enforcement does not inadvertently discriminate against users with different technological capabilities.

Finally, the \textbf{concentration of trust determination} in system administrators who define channel classifications creates potential for abuse. Organizations deploying NIUC systems must establish clear governance frameworks for trust boundary definitions and provide mechanisms for users to understand and potentially contest channel classification decisions that affect their content processing.

\section{Conclusion}

We have presented \textbf{Non-Interactive Universal Computing (NIUC)}, the first formal framework for deterministic enforcement of indirect prompt injection resistance in Large Language Model applications. Our approach addresses the critical gap between probabilistic threat detection and mathematical security guarantees through a small, verifiable runtime system that provides cryptographic attestation of computation integrity.

Our implementation demonstrates that effective LLM security can be achieved through a \textbf{compact, auditable codebase} (224 LOC checker) that operates deterministically without machine learning dependencies or external services. The NIUC checker enforces character-level provenance tracking and imperative isolation through Unicode-resistant normalization and pattern-based detection, providing mathematical guarantees that no untrusted imperatives reach side-effectful operations.

The \textbf{I\textsuperscript{2}-Bench-Lite benchmark} we developed fills a critical gap in the research community by providing standardized evaluation methodology for indirect prompt injection defenses. Our 48 scenarios across six attack categories, each paired with benign twins, enable precise measurement of both security effectiveness and utility preservation. This public benchmark enables reproducible comparative studies and establishes performance baselines for future research.

Our evaluation validates the effectiveness of the NIUC approach, achieving \textbf{8.3\% Attack Success Rate} in block mode while our certified-rewrite mode successfully neutralizes 100\% of attacks with \textbf{0.0\% False Positive Rate}. The system operates with \textbf{0.2ms processing latency}—300× faster than our research target—demonstrating that mathematical security enforcement can be achieved without performance degradation. These results confirm that cryptographic proof-carrying approaches can replace probabilistic detection systems while providing superior guarantees about security properties.

The NIUC framework enables trustworthy composition of LLM applications across organizational boundaries through per-response certificates, addressing critical requirements for privacy-preserving computation in enterprise and federated learning contexts. By providing formal mathematical guarantees rather than statistical risk assessments, NIUC opens new possibilities for deploying LLMs in security-critical applications previously considered too risky for AI integration.


