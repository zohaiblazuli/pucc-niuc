#!/usr/bin/env python3
"""
Run 100-scenario benchmark with all 5 working models.
"""

import os
import sys
from multi_model_benchmark import MultiModelBenchmark

def run_100_scenario_benchmark():
    """Run the 100-scenario benchmark."""
    print("ğŸš€ STARTING 100-SCENARIO MULTI-MODEL BENCHMARK")
    print("=" * 70)
    
    # Check if scenarios file exists
    scenarios_file = "scenarios_100.jsonl"
    if not os.path.exists(scenarios_file):
        print(f"âŒ Scenarios file not found: {scenarios_file}")
        print("Please run generate_100_scenarios.py first")
        return
    
    # Initialize benchmark
    benchmark = MultiModelBenchmark()
    
    # Get available models
    available_models = benchmark.get_available_models()
    print(f"ğŸ“Š Available models: {available_models}")
    print(f"ğŸ¤– Testing {len(available_models)} models")
    print(f"ğŸ“‹ Scenarios file: {scenarios_file}")
    print()
    
    # Run the benchmark
    try:
        results = benchmark.run_full_benchmark()
        print("\nğŸ‰ 100-SCENARIO BENCHMARK COMPLETED SUCCESSFULLY!")
        print("âœ… All models tested")
        print("âœ… Real API validation completed")
        print("âœ… Comprehensive results generated")
        
    except Exception as e:
        print(f"âŒ Benchmark failed: {e}")
        return

if __name__ == "__main__":
    run_100_scenario_benchmark()

