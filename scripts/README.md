# PCC-NIUC Reproducible Evaluation Scripts

This directory contains scripts for running comprehensive, reproducible evaluations of the PCC-NIUC system against the I²-Bench-Lite benchmark suite.

## Scripts Overview

### `repro.sh` (Unix/Linux/macOS)
Bash script for Unix-like systems that runs the complete evaluation pipeline.

### `repro.ps1` (Windows)  
PowerShell script for Windows systems with identical functionality.

### `run_benchmarks.py` (Core Logic)
Python script that performs the actual benchmarking across three modes:
- **Baseline**: No gate protection (simulates unprotected LLM)
- **Block**: Strict violation blocking
- **Rewrite**: Neutralization with re-verification

## Usage

### Windows (PowerShell)
```powershell
# Use mock model (fast testing)
.\scripts\repro.ps1 -ModelType mock

# Use local quantized 7B model  
.\scripts\repro.ps1 -ModelType local

# Use API model (requires OPENAI_API_KEY env var)
.\scripts\repro.ps1 -ModelType api
```

### Unix/Linux/macOS (Bash)
```bash
# Use mock model
MODEL_TYPE=mock ./scripts/repro.sh

# Use local quantized 7B model
MODEL_TYPE=local ./scripts/repro.sh  

# Use API model
MODEL_TYPE=api ./scripts/repro.sh
```

### Direct Python Execution
```bash
python scripts/run_benchmarks.py --model mock --scenarios bench/scenarios.jsonl --results-dir results --timestamp $(date +%Y%m%d_%H%M%S)
```

## Output Files

The scripts generate timestamped output files in the `results/` directory:

- **`metrics_TIMESTAMP.csv`** - Detailed per-scenario results with latency and cost data
- **`summary_TIMESTAMP.md`** - Executive summary with tables and target achievement analysis

## Metrics Computed

### Primary Metrics
- **ASR** (Attack Success Rate): Percentage of attacks that succeed (lower is better)
- **FPR** (False Positive Rate): Percentage of benign content incorrectly blocked (lower is better)
- **UtilityΔ**: Performance degradation percentage compared to baseline (higher is better)
- **Latency**: Average processing time per scenario in milliseconds
- **Cost**: Estimated API costs for model inference (if applicable)

### Research Targets
Based on PCC-NIUC research hypotheses:
1. **ASR ≤10%**: Attack success rate under 10%
2. **FPR <2%**: False positive rate under 2%
3. **UtilityΔ ≥-3%**: Utility degradation no worse than 3%
4. **Latency ≤60ms**: Processing overhead under 60ms per scenario
5. **≥10× Cost Reduction**: 10x or better cost savings vs baseline (API models only)

## Interpreting Results

### ASR (Attack Success Rate) Notes
- **Baseline Mode**: Expected to be 100% (no protection)
- **Block Mode**: Should be low (attacks blocked)
- **Rewrite Mode**: May appear high (100%) but this indicates **successful neutralization**

**Important**: In rewrite mode, ASR of 100% means attacks are being **neutralized and allowed safely**, not that they're succeeding maliciously. The attacks are detected, neutralized (imperatives replaced with `[NEUTRALIZED:verb]`), re-verified to ensure safety, and then allowed to proceed. This is the intended behavior.

### Mode Comparison
- **Baseline**: No security (all attacks succeed, all benign passes)
- **Block**: High security, may block some benign content
- **Rewrite**: Balanced approach - neutralize threats while preserving utility

## Benchmark Suite (I²-Bench-Lite)

The evaluation uses 48 scenarios across 6 categories:
1. **RAG HTML/alt-text** (8 scenarios): HTML injection attacks
2. **Tool-log suggestions** (8 scenarios): API response manipulation
3. **Code-fence injection** (8 scenarios): Code execution markers
4. **Citation/footnote tricks** (8 scenarios): False authority references
5. **Multilingual/emoji/zero-width** (8 scenarios): Unicode evasion techniques
6. **Multi-turn carryover** (8 scenarios): Conversation context poisoning

Each category includes 4 attack scenarios and 4 benign twin scenarios for comprehensive evaluation.

## Reproducibility

The scripts ensure reproducible results by:
- Using deterministic NIUC checker implementation
- Fixed random seeds where applicable
- Consistent normalization and processing pipelines
- Standardized timing methodology
- Clean environment setup and dependency management

## Dependencies

- Python 3.8+
- PCC-NIUC package installed in development mode (`pip install -e .`)
- For API models: Valid API key in `OPENAI_API_KEY` environment variable
- For local models: Model files in expected locations

## Troubleshooting

### Common Issues
- **Import errors**: Ensure `pip install -e .` was run successfully
- **Unicode issues**: Use UTF-8 encoding for all text files
- **Permission errors**: Ensure write access to `results/` directory
- **Model loading failures**: Check model paths and API keys

### Platform-Specific Notes
- **Windows**: Use `repro.ps1` for best compatibility
- **Unix/Linux/macOS**: Use `repro.sh` with bash
- **Any platform**: Direct Python execution always works

---

*Generated by PCC-NIUC project - Privacy-Preserving Computing with Non-Interactive Universal Computing*
